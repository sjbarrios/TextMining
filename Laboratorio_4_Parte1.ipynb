{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad Galileo\n",
    "\n",
    "Text Mining and Image Recognition\n",
    "\n",
    "PAPD - Sección V\n",
    "\n",
    "Sergio José Barrios Martínez\n",
    "\n",
    "Carnet No. 19012765\n",
    "\n",
    "## Laboratorio No. 2\n",
    "\n",
    "\n",
    "***Instrucciones:*** \n",
    "A continuación verá una lista de ejercicios que debe completar para poder entregar el laboratorio #2. Al analizar, todos sus archivos deben estar contenidos en un archivo lab2-sucarnet.zip. Este archivo debe entregarlo en el link del GES. Por favor cree una carpeta para cada\n",
    "ejercicio que usted realice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import spacy.cli\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problema #1:**\n",
    "Utilizando el código trabajado durante la sesión de laboratorio construya una función que le permita realizar la clasificación de un texto aleatorio proporcinado como argumento a dicha función. Su función debe utilzar los procedimientos y modelos de Machine Learning que se generaron en clase, si usted desea puede reescribir los procedimientos para volverlos más eficientes, el objetivo es que se pueda realizar la predicción de ham o spam para cualquier string dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Clasificación de Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_classifier(corpus,modelo):\n",
    "    \n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    \n",
    "    # Normalización\n",
    "    newCorpus = []\n",
    "    for doc in corpus:\n",
    "        newCorpus.append(re.sub(r'[^a-zA-Z0-9\\s{1}áéíóúü]','',doc).lower().strip().rstrip('\\n').rstrip('\\r\\n'))\n",
    "    corpus = newCorpus\n",
    "    \n",
    "    # Tokenización\n",
    "    documents = []\n",
    "    for doc in corpus:\n",
    "        documents.append(nlp(doc))\n",
    "    corpus = documents\n",
    "        \n",
    "    # Remoción de StopWords\n",
    "    documents = []\n",
    "    for doc in corpus:\n",
    "        s = \"\"\n",
    "        for token in doc:\n",
    "            if(token.is_stop == False):\n",
    "                s = s + token.text + \" \"\n",
    "        documents.append(s.strip())\n",
    "    corpus = documents\n",
    "        \n",
    "    # Stemming - Lematización\n",
    "    documents = []\n",
    "    for doc in corpus:\n",
    "        documents.append(nlp(doc))\n",
    "    corpus = documents\n",
    "\n",
    "    documents = []\n",
    "    for doc in corpus:\n",
    "        s = \"\"\n",
    "        for token in doc:\n",
    "            s = s + token.lemma_ + \" \"\n",
    "        documents.append(s.strip())\n",
    "    corpus = documents\n",
    "    \n",
    "    # String de Elementos Unicos\n",
    "    strCorpus = \"\"\n",
    "    for palabra in corpus:\n",
    "        strCorpus = strCorpus + palabra + \" \"\n",
    "    strCorpus = strCorpus.strip().split(' ')\n",
    "    setCorpus = set(strCorpus)\n",
    "    \n",
    "    # Creación del Dataframe para el Modelo\n",
    "    corpusCols = list(setCorpus)\n",
    "    corpusRows = range(0,len(corpus))\n",
    "    outDf = pd.DataFrame(index=corpusRows, columns=corpusCols)\n",
    "    outDf = outDf.fillna(0)\n",
    "    \n",
    "    # Cálculo de TF\n",
    "    documents = []\n",
    "    for doc in corpus:\n",
    "        documents.append(nlp(doc))\n",
    "    corpus = documents\n",
    "    \n",
    "    for index, doc in enumerate(corpus):\n",
    "        bagOfWordsLen = len(doc)\n",
    "        \n",
    "        for word in doc:\n",
    "            try:\n",
    "                colIndex = list(outDf.columns).index(word.text)\n",
    "                outDf.iloc[index,colIndex] = outDf.iloc[index,colIndex] + 1\n",
    "            except:\n",
    "                pass\n",
    "      \n",
    "        if bagOfWordsLen != 0:\n",
    "            outDf.iloc[index,:] = outDf.iloc[index,:]/bagOfWordsLen\n",
    "        else:\n",
    "            outDf.iloc[index,:] = 0\n",
    "    tfMatrix = outDf\n",
    "    \n",
    "    # Cálculo de IDF\n",
    "    N = outDf.shape[0]\n",
    "    valX = (N/outDf.astype(bool).sum(axis=0))\n",
    "    corpusIDF = pd.Series(np.log(valX))\n",
    "    \n",
    "    # Corpus TF - IDF\n",
    "    tfIdfCorpus = tfMatrix.mul(corpusIDF,axis=1)\n",
    "    tfIdfCorpus = tfIdfCorpus.fillna(0)\n",
    "    tfIdfCorpus\n",
    "    \n",
    "    \n",
    "    # Features del modelo\n",
    "    filename = 'features.sav'\n",
    "    features = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    # Creación del Dataframe para el Modelo\n",
    "    observacionesCols = list(features)\n",
    "    columnas = len(observacionesCols)\n",
    "    observacionesRows = range(0,len(tfIdfCorpus))\n",
    "    dataframeX = pd.DataFrame(0.0,index=observacionesRows, columns=observacionesCols)\n",
    "    \n",
    "    # Se actualiza el Dataframe de Predicción con los datos\n",
    "    for fila in range(0,tfIdfCorpus.shape[0]):\n",
    "        for caracteristica in tfIdfCorpus.columns:    \n",
    "            i=0\n",
    "            for columna in dataframeX.columns:            \n",
    "                if caracteristica == columna:\n",
    "                    dataframeX.loc[fila].at[caracteristica]=tfIdfCorpus.loc[fila].at[caracteristica]\n",
    "                i+=1\n",
    "    \n",
    "    # Predicciones dependiendo del modelo\n",
    "    if modelo == \"SVM\":\n",
    "        filename = 'SVM.sav'\n",
    "        svmc = pickle.load(open(filename, 'rb'))\n",
    "        y_preds = svmc.predict(dataframeX)\n",
    "    else:\n",
    "        filename = 'random_forest.sav'\n",
    "        rfc = pickle.load(open(filename, 'rb'))\n",
    "        y_preds = rfc.predict(dataframeX)\n",
    "           \n",
    "    return y_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba sobre muestra aleatoria Dataset Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>ham</td>\n",
       "      <td>Jaja ... puedo ... Pero voy a cenar con mi pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hola, Según la petición de &amp; lt; # &amp; gt; Rs.5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>spam</td>\n",
       "      <td>Tenido sus teléfonos 11 meses o más? T R derec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>ham</td>\n",
       "      <td>Bastante tarde lar ... Ard 12 de todos modos y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>ham</td>\n",
       "      <td>Haciendo mi maestría. Cuando va a comprar un b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>spam</td>\n",
       "      <td>Para ur oportunidad de ganar un dinero en efec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>ham</td>\n",
       "      <td>O simplemente hacer que 6times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>spam</td>\n",
       "      <td>T está suscrito a la mejor de servicios de con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok Ya, vikky vl c witin &amp; lt; # &amp; gt; minutos ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "1345   ham  Jaja ... puedo ... Pero voy a cenar con mi pri...\n",
       "1336   ham  Hola, Según la petición de & lt; # & gt; Rs.5 ...\n",
       "1497  spam  Tenido sus teléfonos 11 meses o más? T R derec...\n",
       "387    ham  Bastante tarde lar ... Ard 12 de todos modos y...\n",
       "992    ham  Haciendo mi maestría. Cuando va a comprar un b...\n",
       "1874  spam  Para ur oportunidad de ganar un dinero en efec...\n",
       "489    ham                     O simplemente hacer que 6times\n",
       "1601  spam  T está suscrito a la mejor de servicios de con...\n",
       "132    ham  ok Ya, vikky vl c witin & lt; # & gt; minutos ..."
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strSpamDB = 'es_spam.csv'\n",
    "spamDB = pd.read_csv(strSpamDB, sep=',',names=['label','message'])\n",
    "spamDB = spamDB.iloc[1:]\n",
    "ham = spamDB[spamDB['label'] == 'ham']\n",
    "spam = spamDB[spamDB['label'] == 'spam']\n",
    "ham = ham.sample(2*spam.shape[0])\n",
    "ham.shape, spam.shape\n",
    "dataset = ham.append(spam,ignore_index=True)\n",
    "dataset = dataset.sample(9)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PruebaRandomForest=spam_classifier(dataset.message,\"RandomForest\")\n",
    "PruebaRandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción con SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PruebaSVM=spam_classifier(dataset.message,\"SVM\")\n",
    "PruebaSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con Texto Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto1 = \"Oferta de vacaciones de verano gratis\"\n",
    "texto2 = \"Gane dinero sin esfuerzo\"\n",
    "texto3 = \"Buenas tardes Ingeniero\"\n",
    "data = {'Texto':  [texto1,texto2,texto3]}\n",
    "daf = pd.DataFrame (data, columns = ['Texto'])\n",
    "TextoAleatorioSVM=spam_classifier(daf.Texto,\"SVM\")\n",
    "TextoAleatorioSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
